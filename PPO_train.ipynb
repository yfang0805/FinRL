{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from env import StockLearningEnv\n",
    "import config\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据缓存\n",
      "数据缓存成功!\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data_file/train.csv')\n",
    "e_train_gym = StockLearningEnv(df = df, **config.ENV_TRAIN_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devic: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Beta,Normal\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('devic:', device)\n",
    "\n",
    "class BetaActor(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, net_width):\n",
    "\t\tsuper(BetaActor, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, net_width)\n",
    "\t\tself.l2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.alpha_head = nn.Linear(net_width, action_dim)\n",
    "\t\tself.beta_head = nn.Linear(net_width, action_dim)\n",
    "\t\tself.attention1 = nn.MultiheadAttention(net_width, 1, batch_first=True)\n",
    "\t\tself.attention2 = nn.MultiheadAttention(net_width, 1, batch_first=True)\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = torch.tanh(self.l1(state))\n",
    "\t\ta = torch.tanh(self.l2(a))\n",
    "\n",
    "\t\talpha = F.softplus(self.alpha_head(a)) + 1.0\n",
    "\t\tbeta = F.softplus(self.beta_head(a)) + 1.0\n",
    "\n",
    "\t\treturn alpha,beta\n",
    "\n",
    "\tdef get_dist(self,state):\n",
    "\t\talpha,beta = self.forward(state)\n",
    "\t\tdist = Beta(alpha, beta)\n",
    "\t\treturn dist\n",
    "\n",
    "\tdef dist_mode(self,state):\n",
    "\t\talpha, beta = self.forward(state)\n",
    "\t\tmode = (alpha) / (alpha + beta)\n",
    "\t\treturn mode\n",
    "\n",
    "class GaussianActor_musigma(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, net_width):\n",
    "\t\tsuper(GaussianActor_musigma, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, net_width)\n",
    "\t\tself.l2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.mu_head = nn.Linear(net_width, action_dim)\n",
    "\t\tself.sigma_head = nn.Linear(net_width, action_dim)\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = torch.tanh(self.l1(state))\n",
    "\t\ta = torch.tanh(self.l2(a))\n",
    "\t\tmu = torch.sigmoid(self.mu_head(a))\n",
    "\t\tsigma = F.softplus( self.sigma_head(a) )\n",
    "\t\treturn mu,sigma\n",
    "\n",
    "\tdef get_dist(self, state):\n",
    "\t\tmu,sigma = self.forward(state)\n",
    "\t\tdist = Normal(mu,sigma)\n",
    "\t\treturn dist\n",
    "\n",
    "class GaussianActor_mu(nn.Module):\n",
    "\tdef __init__(self, state_dim, action_dim, net_width, log_std=0):\n",
    "\t\tsuper(GaussianActor_mu, self).__init__()\n",
    "\n",
    "\t\tself.l1 = nn.Linear(state_dim, net_width)\n",
    "\t\tself.l2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.mu_head = nn.Linear(net_width, action_dim)\n",
    "\t\tself.mu_head.weight.data.mul_(0.1)\n",
    "\t\tself.mu_head.bias.data.mul_(0.0)\n",
    "\n",
    "\t\tself.action_log_std = nn.Parameter(torch.ones(1, action_dim) * log_std)\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\ta = torch.relu(self.l1(state))\n",
    "\t\ta = torch.relu(self.l2(a))\n",
    "\t\tmu = torch.sigmoid(self.mu_head(a))\n",
    "\t\treturn mu\n",
    "\n",
    "\tdef get_dist(self,state):\n",
    "\t\tmu = self.forward(state)\n",
    "\t\taction_log_std = self.action_log_std.expand_as(mu)\n",
    "\t\taction_std = torch.exp(action_log_std)\n",
    "\n",
    "\t\tdist = Normal(mu, action_std)\n",
    "\t\treturn dist\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, state_dim,net_width):\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\t\tself.net_width = net_width\n",
    "\t\t\n",
    "\t\tself.C1 = nn.Linear(state_dim, net_width)\n",
    "\t\tself.C2 = nn.Linear(net_width, net_width)\n",
    "\t\tself.C3 = nn.Linear(net_width, 1)\n",
    "\n",
    "\t\tself.attention1 = nn.MultiheadAttention(net_width, 1, batch_first=True)\n",
    "\t\tself.attention2 = nn.MultiheadAttention(net_width, 1, batch_first=True)\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\tv = torch.tanh(self.C1(state))\n",
    "\n",
    "\t\tv = v.view(-1, 1, self.net_width)\n",
    "\n",
    "\t\tv, attn_output_weights = self.attention1(v, v, v)\n",
    "\n",
    "\t\tv = v.view(-1, self.net_width)\n",
    "\n",
    "\n",
    "\t\tv = torch.tanh(self.C2(v))\n",
    "\t\tv = v.view(-1, 1, self.net_width)\n",
    "\t\tv, attn_output_weights = self.attention2(v, v, v)\n",
    "\t\tv = v.view(-1, self.net_width)\n",
    "\n",
    "\t\tv = self.C3(v)\n",
    "\t\treturn v\n",
    "\n",
    "\n",
    "\n",
    "class Attention_PPO(object):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tstate_dim,\n",
    "\t\taction_dim,\n",
    "\t\tenv_with_Dead,\n",
    "\t\tgamma=0.99,\n",
    "\t\tlambd=0.95,\n",
    "\t\tclip_rate=0.2,\n",
    "\t\tK_epochs=10,\n",
    "\t\tnet_width=256,\n",
    "\t\ta_lr=3e-4,\n",
    "\t\tc_lr=3e-4,\n",
    "\t\tl2_reg = 1e-3,\n",
    "\t\tdist='Beta',\n",
    "\t\ta_optim_batch_size = 64,\n",
    "\t\tc_optim_batch_size = 64,\n",
    "\t\tentropy_coef = 0,\n",
    "\t\tentropy_coef_decay = 0.9998\n",
    "\t):\n",
    "\t\tif dist == 'Beta':\n",
    "\t\t\tself.actor = BetaActor(state_dim, action_dim, net_width).to(device)\n",
    "\t\telif dist == 'GS_ms':\n",
    "\t\t\tself.actor = GaussianActor_musigma(state_dim, action_dim, net_width).to(device)\n",
    "\t\telif dist == 'GS_m':\n",
    "\t\t\tself.actor = GaussianActor_mu(state_dim, action_dim, net_width).to(device)\n",
    "\t\telse: print('Dist Error')\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=a_lr)\n",
    "\t\tself.dist = dist\n",
    "\n",
    "\t\tself.critic = Critic(state_dim, net_width).to(device)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=c_lr)\n",
    "\n",
    "\t\tself.env_with_Dead = env_with_Dead\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.clip_rate = clip_rate\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.lambd = lambd\n",
    "\t\tself.clip_rate = clip_rate\n",
    "\t\tself.K_epochs = K_epochs\n",
    "\t\tself.data = []\n",
    "\t\tself.l2_reg = l2_reg\n",
    "\t\tself.a_optim_batch_size = a_optim_batch_size\n",
    "\t\tself.c_optim_batch_size = c_optim_batch_size\n",
    "\t\tself.entropy_coef = entropy_coef\n",
    "\t\tself.entropy_coef_decay = entropy_coef_decay\n",
    "\n",
    "\tdef select_action(self, state):#only used when interact with the env\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\t\tdist = self.actor.get_dist(state)\n",
    "\t\t\ta = dist.sample()\n",
    "\t\t\ta = torch.clamp(a, 0, 1)\n",
    "\t\t\tlogprob_a = dist.log_prob(a).cpu().numpy().flatten()\n",
    "\t\t\treturn a.cpu().numpy().flatten(), logprob_a\n",
    "\n",
    "\tdef evaluate(self, state):#only used when evaluate the policy.Making the performance more stable\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "\t\t\tif self.dist == 'Beta':\n",
    "\t\t\t\ta = self.actor.dist_mode(state)\n",
    "\t\t\tif self.dist == 'GS_ms':\n",
    "\t\t\t\ta,b = self.actor(state)\n",
    "\t\t\tif self.dist == 'GS_m':\n",
    "\t\t\t\ta = self.actor(state)\n",
    "\t\t\treturn a.cpu().numpy().flatten(),0.0\n",
    "\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tself.entropy_coef*=self.entropy_coef_decay\n",
    "\t\ts, a, r, s_prime, logprob_a, done_mask, dw_mask = self.make_batch()\n",
    "\n",
    "\n",
    "\t\t''' Use TD+GAE+LongTrajectory to compute Advantage and TD target'''\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tvs = self.critic(s)\n",
    "\t\t\tvs_ = self.critic(s_prime)\n",
    "\n",
    "\t\t\t'''dw for TD_target and Adv'''\n",
    "\t\t\tdeltas = r + self.gamma * vs_ * (1 - dw_mask) - vs\n",
    "\n",
    "\t\t\tdeltas = deltas.cpu().flatten().numpy()\n",
    "\t\t\tadv = [0]\n",
    "\n",
    "\t\t\t'''done for GAE'''\n",
    "\t\t\tfor dlt, mask in zip(deltas[::-1], done_mask.cpu().flatten().numpy()[::-1]):\n",
    "\t\t\t\tadvantage = dlt + self.gamma * self.lambd * adv[-1] * (1 - mask)\n",
    "\t\t\t\tadv.append(advantage)\n",
    "\t\t\tadv.reverse()\n",
    "\t\t\tadv = copy.deepcopy(adv[0:-1])\n",
    "\t\t\tadv = torch.tensor(adv).unsqueeze(1).float().to(device)\n",
    "\t\t\ttd_target = adv + vs\n",
    "\t\t\tadv = (adv - adv.mean()) / ((adv.std()+1e-4))  #sometimes helps\n",
    "\n",
    "\n",
    "\t\t\"\"\"Slice long trajectopy into short trajectory and perform mini-batch PPO update\"\"\"\n",
    "\t\ta_optim_iter_num = int(math.ceil(s.shape[0] / self.a_optim_batch_size))\n",
    "\t\tc_optim_iter_num = int(math.ceil(s.shape[0] / self.c_optim_batch_size))\n",
    "\t\tfor i in range(self.K_epochs):\n",
    "\n",
    "\t\t\t#Shuffle the trajectory, Good for training\n",
    "\t\t\tperm = np.arange(s.shape[0])\n",
    "\t\t\tnp.random.shuffle(perm)\n",
    "\t\t\tperm = torch.LongTensor(perm).to(device)\n",
    "\t\t\ts, a, td_target, adv, logprob_a = \\\n",
    "\t\t\t\ts[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), logprob_a[perm].clone()\n",
    "\n",
    "\t\t\t'''update the actor'''\n",
    "\t\t\tfor i in range(a_optim_iter_num):\n",
    "\t\t\t\tindex = slice(i * self.a_optim_batch_size, min((i + 1) * self.a_optim_batch_size, s.shape[0]))\n",
    "\t\t\t\tdistribution = self.actor.get_dist(s[index])\n",
    "\t\t\t\tdist_entropy = distribution.entropy().sum(1, keepdim=True)\n",
    "\t\t\t\tlogprob_a_now = distribution.log_prob(a[index])\n",
    "\t\t\t\tratio = torch.exp(logprob_a_now.sum(1,keepdim=True) - logprob_a[index].sum(1,keepdim=True))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "\t\t\t\tsurr1 = ratio * adv[index]\n",
    "\t\t\t\tsurr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n",
    "\t\t\t\ta_loss = -torch.min(surr1, surr2) - self.entropy_coef * dist_entropy\n",
    "\n",
    "\t\t\t\tself.actor_optimizer.zero_grad()\n",
    "\t\t\t\ta_loss.mean().backward()\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n",
    "\t\t\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\t\t'''update the critic'''\n",
    "\t\t\tfor i in range(c_optim_iter_num):\n",
    "\t\t\t\tindex = slice(i * self.c_optim_batch_size, min((i + 1) * self.c_optim_batch_size, s.shape[0]))\n",
    "\t\t\t\tc_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n",
    "\t\t\t\tfor name,param in self.critic.named_parameters():\n",
    "\t\t\t\t\tif 'weight' in name:\n",
    "\t\t\t\t\t\tc_loss += param.pow(2).sum() * self.l2_reg\n",
    "\n",
    "\t\t\t\tself.critic_optimizer.zero_grad()\n",
    "\t\t\t\tc_loss.backward()\n",
    "\t\t\t\tself.critic_optimizer.step()\n",
    "\n",
    "\n",
    "\tdef make_batch(self):\n",
    "\t\ts_lst, a_lst, r_lst, s_prime_lst, logprob_a_lst, done_lst, dw_lst = [], [], [], [], [], [], []\n",
    "\t\tfor transition in self.data:\n",
    "\t\t\ts, a, r, s_prime, logprob_a, done, dw = transition\n",
    "\n",
    "\t\t\ts_lst.append(s)\n",
    "\t\t\ta_lst.append(a)\n",
    "\t\t\tlogprob_a_lst.append(logprob_a)\n",
    "\t\t\tr_lst.append([r])\n",
    "\t\t\ts_prime_lst.append(s_prime)\n",
    "\t\t\tdone_lst.append([done])\n",
    "\t\t\tdw_lst.append([dw])\n",
    "\n",
    "\t\tif not self.env_with_Dead:\n",
    "\t\t\t'''Important!!!'''\n",
    "\t\t\t# env_without_DeadAndWin: deltas = r + self.gamma * vs_ - vs\n",
    "\t\t\t# env_with_DeadAndWin: deltas = r + self.gamma * vs_ * (1 - dw) - vs\n",
    "\t\t\tdw_lst = (np.array(dw_lst)*False).tolist()\n",
    "\n",
    "\t\tself.data = [] #Clean history trajectory\n",
    "\n",
    "\t\t'''list to tensor'''\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ts, a, r, s_prime, logprob_a, done_mask, dw_mask = \\\n",
    "\t\t\t\ttorch.tensor(s_lst, dtype=torch.float).to(device), \\\n",
    "\t\t\t\ttorch.tensor(a_lst, dtype=torch.float).to(device), \\\n",
    "\t\t\t\ttorch.tensor(r_lst, dtype=torch.float).to(device), \\\n",
    "\t\t\t\ttorch.tensor(s_prime_lst, dtype=torch.float).to(device), \\\n",
    "\t\t\t\ttorch.tensor(logprob_a_lst, dtype=torch.float).to(device), \\\n",
    "\t\t\t\ttorch.tensor(done_lst, dtype=torch.float).to(device), \\\n",
    "\t\t\t\ttorch.tensor(dw_lst, dtype=torch.float).to(device),\n",
    "\n",
    "\n",
    "\t\treturn s, a, r, s_prime, logprob_a, done_mask, dw_mask\n",
    "\n",
    "\n",
    "\tdef put_data(self, transition):\n",
    "\t\tself.data.append(transition)\n",
    "\n",
    "\tdef save(self,episode):\n",
    "\t\ttorch.save(self.critic.state_dict(), \"./attention_ppo/ppo_critic{}.pth\".format(episode))\n",
    "\t\ttorch.save(self.actor.state_dict(), \"./attention_ppo/ppo_actor{}.pth\".format(episode))\n",
    "\n",
    "\n",
    "\tdef load(self,episode):\n",
    "\t\tself.critic.load_state_dict(torch.load(\"./attention_ppo/ppo_critic{}.pth\".format(episode)))\n",
    "\t\tself.actor.load_state_dict(torch.load(\"./attention_ppo/ppo_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PPO import PPO\n",
    "# from Attention_PPO import Attention_PPO\n",
    "kwargs = {\n",
    "    \"state_dim\": 801,\n",
    "    \"action_dim\": 50,\n",
    "    \"env_with_Dead\":True\n",
    "}\n",
    "model = Attention_PPO(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_horizon = 100\n",
    "traj_lenth = 0\n",
    "total_steps = 0\n",
    "Max_train_steps = 50000\n",
    "save_interval = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_14584\\2622111708.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[1;31m# print(np.shape(s))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m         \u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogprob_a\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mselect_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_14584\\1379624915.py\u001B[0m in \u001B[0;36mselect_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    170\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mselect_action\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;31m#only used when interact with the env\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    171\u001B[0m                 \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 172\u001B[1;33m                         \u001B[0mstate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFloatTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    173\u001B[0m                         \u001B[0mdist\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mactor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_dist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m                         \u001B[0ma\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdist\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "while total_steps < Max_train_steps:\n",
    "    s, done, steps, ep_r = e_train_gym.reset(), False, 0, 0\n",
    "\n",
    "    '''Interact & trian'''\n",
    "    while not done:\n",
    "        traj_lenth += 1\n",
    "        # print('s:', s)\n",
    "        s = np.array(s)\n",
    "        # print(np.shape(s))\n",
    "        a, logprob_a = model.select_action(s)\n",
    "\n",
    "        \n",
    "        s_prime, r, done, info = e_train_gym.step(a)\n",
    "       \n",
    "\n",
    "        '''distinguish done between dead|win(dw) and reach env._max_episode_steps(rmax); done = dead|win|rmax'''\n",
    "        '''dw for TD_target and Adv; done for GAE'''\n",
    "        if done :\n",
    "            dw = True\n",
    "        else:\n",
    "            dw = False\n",
    "\n",
    "        model.put_data((s, a, r, s_prime, logprob_a, done, dw))\n",
    "        s = s_prime\n",
    "        ep_r += r\n",
    "\n",
    "        '''update if its time'''\n",
    "        \n",
    "        if traj_lenth % T_horizon == 0:\n",
    "            model.train()\n",
    "            traj_lenth = 0\n",
    "\n",
    "        total_steps += 1\n",
    "        \n",
    "        '''save model'''\n",
    "        if total_steps % save_interval==0:\n",
    "            model.save(total_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "947c325c4eb95a9946ab668956b3cfc6d347696304b060e14630bd99381a3fa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
